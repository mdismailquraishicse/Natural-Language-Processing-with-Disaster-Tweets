{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73ee5a8-17c1-487b-8737-bd835741cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6122aa8-8675-412e-a7a1-b6777eb023b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------test--------------------\n",
      "shape: (3263, 4)\n",
      "duplicates: 0\n",
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "\n",
      "\n",
      "--------------------train--------------------\n",
      "shape: (7613, 5)\n",
      "duplicates: 0\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n"
     ]
    }
   ],
   "source": [
    "######################### To Configure Kaggle #########################\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "######################### Load datasets #########################\n",
    "path_train = \"nlp-getting-started/train.csv\"\n",
    "path_test = \"nlp-getting-started/test.csv\"\n",
    "train = pd.read_csv(path_train)\n",
    "test = pd.read_csv(path_test)\n",
    "print(\"-\"*20+\"test\"+\"-\"*20)\n",
    "print(\"shape:\",test.shape)\n",
    "print(\"duplicates:\",test.duplicated().sum())\n",
    "print(test.head(2))\n",
    "print()\n",
    "print()\n",
    "print(\"-\"*20+\"train\"+\"-\"*20)\n",
    "print(\"shape:\",train.shape)\n",
    "print(\"duplicates:\",test.duplicated().sum())\n",
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2076dda-5dff-4756-8e72-cd8d6f610961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9459715639810427"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_mentions(text):\n",
    "    pattern = re.compile(r\"@\\w+\")\n",
    "    text = pattern.sub(\"users\",text)\n",
    "    return text\n",
    "    \n",
    "def handle_hashtags(text):\n",
    "    pattern = re.compile(r\"#(\\w+)\")\n",
    "    text = pattern.sub(r\"\\1\",text)\n",
    "    return text\n",
    "    \n",
    "def handle_url(text):\n",
    "    pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    text = pattern.sub(\"http\", text)\n",
    "    return text\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = handle_url(text)\n",
    "    text = handle_hashtags(text)\n",
    "    text = handle_mentions(text)\n",
    "    return text\n",
    "\n",
    "def preprocess(df):\n",
    "    # df[\"keyword\"] = df[\"keyword\"].fillna(\"keyword_missing\")\n",
    "    # df[\"clean_text\"] = df[\"keyword\"]+\" \"+df[\"text\"]\n",
    "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = preprocess(train.copy())\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    strip_accents= \"unicode\", # café → cafe\n",
    "    sublinear_tf= True,\n",
    "    ngram_range=(1,3),\n",
    "    stop_words=['http', 'users', 'keyword_missing']\n",
    ")\n",
    "vectors_arr = tfidf.fit_transform(df[\"clean_text\"]).toarray()\n",
    "df_vec = pd.DataFrame(vectors_arr, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "df_vec[\"is_keyword\"] = df[\"keyword\"].isna().astype(int)\n",
    "\n",
    "model = RidgeClassifier(alpha=1.0)\n",
    "model.fit(df_vec.values, df[\"target\"].values)\n",
    "\n",
    "pred = model.predict(df_vec.values)\n",
    "f1_score(df[\"target\"].values, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7873cb6c-74b2-4269-86b7-b2daa76d1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(test.copy())\n",
    "vectors_arr = tfidf.transform(df[\"clean_text\"]).toarray()\n",
    "df_vec = pd.DataFrame(vectors_arr, columns=tfidf.get_feature_names_out())\n",
    "df_vec[\"is_keyword\"] = df[\"keyword\"].isna().astype(int)\n",
    "\n",
    "pred = model.predict(df_vec.values)\n",
    "submission = pd.DataFrame()\n",
    "submission[\"id\"] = test[\"id\"]\n",
    "submission[\"target\"] = pred\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38864387-f804-4c79-8ec7-fc4e36b6e206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
